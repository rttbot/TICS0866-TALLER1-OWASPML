# ğŸ§¬ TICS00866 - Taller 1: IA Security Village
## Instrucciones para Obtener la Nota

---

## ğŸ­ **Â¿QuÃ© es el IA Security Village?**

Inspirados en el **Biohacking Village de DEFCON**, donde empresas como **Agfa**, **Medtronic**, **Siemens Healthineers** y **Boston Scientific** llevan sus dispositivos mÃ©dicos para ser evaluados por hackers Ã©ticos, nuestro taller simula un entorno similar:

- **ğŸ¢ "Empresa":** Universidad Adolfo IbÃ¡Ã±ez presenta su sistema Intrusion.Aware
- **ğŸ” "Hackers Ã‰ticos":** Estudiantes del curso TICS00866
- **ğŸ¯ "Objetivo":** Encontrar vulnerabilidades antes del lanzamiento al mercado
- **ğŸ† "Premio":** Contribuir a la seguridad de sistemas de IA para la sociedad

### **ğŸ® Â¿Por quÃ© es Importante este Juego de Roles?**

AsÃ­ como los dispositivos mÃ©dicos pueden causar daÃ±o fÃ­sico si son vulnerables, los **sistemas de IA** pueden causar daÃ±o social, econÃ³mico y de privacidad si no son seguros. Nuestro taller sigue la misma filosofÃ­a:

- **ğŸ” DetecciÃ³n Temprana:** Encontrar vulnerabilidades antes del despliegue
- **ğŸ›¡ï¸ ProtecciÃ³n Social:** Evitar que sistemas inseguros afecten a la sociedad
- **ğŸ¤ ColaboraciÃ³n:** Trabajar con desarrolladores para mejorar la seguridad
- **ğŸ“š EducaciÃ³n:** Crear conciencia sobre vulnerabilidades en IA

**Referencias:**
- [DEFCON Biohacking Village 2024 - Deloitte](https://www.deloitte.com/hu/en/services/risk-advisory/perspectives/defcon-biohacking-village-2024.html)
- [OWASP Machine Learning Security Top 10](https://github.com/owasp/www-project-machine-learning-security-top-10)
- [Biohacking Village Official](https://www.villageb.io/)

---

## ğŸ“‹ **Â¿QuÃ© Debes Hacer para Obtener la Nota?**

### **PASO 1: Acuerdo de Hacking Ã‰tico (OBLIGATORIO)**
- âœ… **Descargar** el documento `S6-2-01-acuerdo-hacking-etico.md`
- âœ… **Completar** todos los campos (nombre, email, carrera, usuario GitHub)
- âœ… **Subir al aula virtual** en la casilla especial "Acuerdo Hacking Ã‰tico"
- âœ… **Nombre del archivo:** `acuerdo-hacking-etico-[TU-NOMBRE].pdf`

### **PASO 2: Acceso al Repositorio GitHub**
- âœ… **Repositorio:** https://github.com/rttbot/TICS0866-TALLER1-OWASPML
- âœ… **Ser agregado como colaborador** por la profesora
- âœ… **Crear carpeta especÃ­fica** segÃºn vulnerabilidad asignada (ML01-ML10)
- âœ… **Subir archivos de entrega** en la carpeta correspondiente

---

## ğŸ“ **Entregables Obligatorios (70% del NPE)**

### **Estructura de Carpetas:**
```
TICS0866-TALLER1-OWASPML/
â”œâ”€â”€ ML01/          # ManipulaciÃ³n de Entrada
â”œâ”€â”€ ML02/          # Envenenamiento de Datos
â”œâ”€â”€ ML03/          # InversiÃ³n de Modelos
â”œâ”€â”€ ML04/          # Inferencia de Pertenencia
â”œâ”€â”€ ML05/          # Robo de Modelos
â”œâ”€â”€ ML06/          # Cadena de Suministro
â”œâ”€â”€ ML07/          # Transfer Learning
â”œâ”€â”€ ML08/          # Sesgo del Modelo
â”œâ”€â”€ ML09/          # Integridad de Salida
â””â”€â”€ ML10/          # Envenenamiento de Modelos
```

### **Archivos por Carpeta (3 archivos obligatorios):**

#### **1. Casos de Prueba de Pentesting**
**Archivo:** `casos-prueba-pentesting-[NOMBRE].md`
- MÃ­nimo **2 casos** para la vulnerabilidad asignada
- Procedimientos paso a paso detallados
- Evidencia esperada

#### **2. AnÃ¡lisis de Vulnerabilidades**
**Archivo:** `analisis-vulnerabilidades-owasp-atlas-[NOMBRE].md`
- ComparaciÃ³n OWASP ML Top 10 vs ATLAS ML
- AnÃ¡lisis de similitudes y diferencias
- Casos de aplicaciÃ³n especÃ­ficos a Intrusion.Aware

#### **3. Informe Ejecutivo Final GRUPAL del Taller**
**Archivo:** `README.md`
- Resumen ejecutivo de hallazgos
- AnÃ¡lisis de vulnerabilidades encontradas
- ComparaciÃ³n con el estudio de MÃ©xico
- Conclusiones y lecciones aprendidas

---

## ğŸ“Š **Criterios de EvaluaciÃ³n**

### **EvaluaciÃ³n TÃ©cnica (50%)**
- **Calidad de casos de prueba:** 20%
- **IdentificaciÃ³n de vulnerabilidades:** 15%
- **AnÃ¡lisis tÃ©cnico:** 15%

### **ExplicaciÃ³n Simple de Vulnerabilidades (25%)**
- **Claridad en la explicaciÃ³n:** 15%
  - **Simplicidad:** 8% - ExplicaciÃ³n accesible para audiencia no tÃ©cnica
  - **Ejemplos PrÃ¡cticos:** 7% - Ejemplos concretos aplicados a Intrusion.Aware
- **Contexto Regional:** 10%
  - **Brecha Identificada:** 5% - ComprensiÃ³n de la brecha en informes regionales
  - **Relevancia Social:** 5% - ExplicaciÃ³n del impacto social de la vulnerabilidad

### **ReflexiÃ³n Grupal sobre Seguridad en IA (20%)**
- **AnÃ¡lisis CrÃ­tico:** 10%
  - **EvaluaciÃ³n de OWASP ML Top 10:** 5% - Â¿EvalÃºa lo que nosotros consideramos como seguridad en IA?
  - **ComparaciÃ³n con ATLAS ML:** 5% - AnÃ¡lisis de similitudes y diferencias
- **Contexto Regional:** 10%
  - **Brecha Identificada:** 5% - ComprensiÃ³n de la brecha en informes regionales
  - **Relevancia Social:** 5% - ExplicaciÃ³n del impacto social de la vulnerabilidad

### **ParticipaciÃ³n (5%)**
- **ColaboraciÃ³n:** 3%
- **Cumplimiento de normas:** 2%

---

## ğŸ¯ **Las 10 Vulnerabilidades que Analizaremos**

BasÃ¡ndonos en el [OWASP Machine Learning Security Top 10](https://mltop10.info/OWASP-Machine-Learning-Security-Top-10.pdf), cada estudiante serÃ¡ asignado a **UNA** de las siguientes vulnerabilidades:

### **ğŸ¯ ML01 - ManipulaciÃ³n de Entrada** 
**Â¿QuÃ© es?** Es como si alguien pudiera cambiar las seÃ±ales de trÃ¡fico para que un auto autÃ³nomo tome la direcciÃ³n equivocada.

**En Intrusion.Aware:** Atacantes alteran datos de entrada para evadir detecciÃ³n.
- **Ejemplo**: Enviar exactamente 999 bytes para evadir regla `sbytes > 1000`
- **Herramientas Kali**: `wfuzz`, `ffuf`, `dirb`

### **ğŸ¯ ML02 - Envenenamiento de Datos**
**Â¿QuÃ© es?** Es como si alguien contaminara la comida de un chef para que aprenda a cocinar mal.

**En Intrusion.Aware:** Atacantes inyectan datos maliciosos en el dataset de entrenamiento.
- **Ejemplo**: Analistas corruptos marcan ataques como normales
- **Impacto**: Decision Trees aprenden reglas incorrectas

### **ğŸ¯ ML03 - InversiÃ³n de Modelos**
**Â¿QuÃ© es?** Es como si alguien pudiera "hacer ingenierÃ­a inversa" de cÃ³mo piensa un experto.

**En Intrusion.Aware:** Atacantes intentan extraer informaciÃ³n del modelo Decision Tree.
- **LimitaciÃ³n**: Decision Trees son intrÃ­nsecamente interpretables
- **Menor impacto** que en modelos black-box

### **ğŸ¯ ML04 - Inferencia de Pertenencia**
**Â¿QuÃ© es?** Es como si alguien pudiera adivinar si una foto especÃ­fica fue usada para entrenar un modelo.

**En Intrusion.Aware:** Atacantes determinan si datos especÃ­ficos estuvieron en entrenamiento.
- **ProtecciÃ³n**: Dataset UNSW-NB15 es pÃºblico y anonimizado

### **ğŸ¯ ML05 - Robo de Modelos**
**Â¿QuÃ© es?** Es como si alguien robara la receta secreta de un chef.

**En Intrusion.Aware:** Atacantes roban el modelo completo para uso malicioso.
- **Bajo riesgo**: Decision Trees son modelos simples y pÃºblicos

### **ğŸ¯ ML06 - Cadena de Suministro**
**Â¿QuÃ© es?** Es como si alguien reemplazara las herramientas de un mecÃ¡nico con herramientas defectuosas.

**En Intrusion.Aware:** Ataques a componentes externos del sistema.
- **Ejemplo**: Compromiso de librerÃ­as de ML o datasets

### **ğŸ¯ ML07 - Transfer Learning**
**Â¿QuÃ© es?** Es como si alguien enseÃ±ara mal a un estudiante que luego enseÃ±a a otros.

**En Intrusion.Aware:** Ataques a modelos pre-entrenados.
- **Ejemplo**: Modelo base comprometido afecta modelos derivados

### **ğŸ¯ ML08 - Sesgo del Modelo**
**Â¿QuÃ© es?** Es como si un juez fuera parcial hacia ciertos grupos de personas.

**En Intrusion.Aware:** Modelos que discriminan ciertos tipos de trÃ¡fico.
- **Ejemplo**: Falsos positivos para ciertos protocolos

### **ğŸ¯ ML09 - Integridad de Salida**
**Â¿QuÃ© es?** Es como si alguien interceptara y cambiara el diagnÃ³stico de un mÃ©dico.

**En Intrusion.Aware:** Atacantes modifican las predicciones del modelo.
- **Ejemplo**: Cambiar "ATAQUE" por "NORMAL" en las alertas

### **ğŸ¯ ML10 - Envenenamiento de Modelos**
**Â¿QuÃ© es?** Es como si alguien modificara directamente el cerebro de un experto.

**En Intrusion.Aware:** Atacantes manipulan los parÃ¡metros del modelo.
- **Ejemplo**: Alterar pesos de Decision Trees para clasificaciones incorrectas

---

## ğŸ“š **Referencias Requeridas**

### **Recursos Principales:**
- [OWASP Machine Learning Security Top 10](https://github.com/owasp/www-project-machine-learning-security-top-10)
- [OWASP ML Top 10 PDF](https://mltop10.info/OWASP-Machine-Learning-Security-Top-10.pdf)
- [OWASP AI Security and Privacy Guide](https://owasp.org/www-project-ai-security-and-privacy-guide/)
- [Ejemplos PrÃ¡cticos de Vulnerabilidades ML](https://www.getastra.com/blog/security-audit/owasp-machine-learning-top-10/)

### **Contexto Regional:**
- [Informe de MÃ©xico - ANCI](https://anci.gob.cl/noticias/mexico-reunion-ciberseguridad-ia/)
- [DEFCON Biohacking Village 2024 - Deloitte](https://www.deloitte.com/hu/en/services/risk-advisory/perspectives/defcon-biohacking-village-2024.html)

### **Recursos Adicionales:**
- [Adversarial Robustness Toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)
- [CleverHans Library](https://github.com/cleverhans-lab/cleverhans)
- [NIST AI Risk Management Framework](https://www.nist.gov/ai/risk-management)

---

## âš ï¸ **Normas del Taller**

- âœ… **Solo diseÃ±o** de casos de prueba (NO implementaciÃ³n)
- âœ… **Trabajo individual** (ENTREGA individual)
- âœ… **Entrega puntual** de todos los materiales
- âœ… **Formato correcto** de archivos
- âœ… **ParticipaciÃ³n activa** durante el taller

---

## ğŸ“ **Contacto**

**Profesora:** Romina Torres  
**Asignatura:** TICS00866 - AuditorÃ­a y Defensa en Sistemas de Inteligencia Artificial  
**InstituciÃ³n:** Universidad Adolfo IbÃ¡Ã±ez

---

**Â¡Ã‰xito en el taller!** ğŸ¯
